# -*- coding: utf-8 -*-
"""
æ°´ä¹¡è¯æ±‡è¿‘ä¹‰è¯åŒ¹é…å·¥å…·ï¼ˆæ”¯æŒå¤–éƒ¨æ–‡ä»¶å¯¼å…¥ï¼‰
éœ€å®‰è£…ï¼špip install pandas sentence-transformers
"""

# ============== éœ€è¦ç”¨æˆ·ä¿®æ”¹çš„éƒ¨åˆ† ==============
# 1. æ–‡ä»¶è·¯å¾„é…ç½®ï¼ˆæ ¹æ®å®é™…æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸€ç§ï¼‰
A_FILE = "æ°´ä¹¡è¯åº“.csv"  # ä¿®æ”¹ä¸º CSV æ–‡ä»¶è·¯å¾„
B_FILE = "output_æ­å·å‰ç¥¥é‡Œè¯åº“.csv"
FILE_TYPE = "csv"  # ä¿®æ”¹ä¸º csv

# 2. è¡¨æ ¼é…ç½®ï¼ˆå¦‚æœä½¿ç”¨excel/csvéœ€è¦è®¾ç½®ï¼‰
COLUMN_NAME = "åŸè¯"  # è¯æ±‡æ‰€åœ¨çš„åˆ—åï¼ˆexcel/csvéœ€è¦ï¼‰
# SHEET_NAME å¯¹äº CSV æ–‡ä»¶ä¸éœ€è¦ï¼Œå¯åˆ é™¤æˆ–æ³¨é‡Šæ‰
# SHEET_NAME = "Sheet1"

# 3. ç›¸ä¼¼åº¦é˜ˆå€¼
THRESHOLD = 0.72
# ==============================================

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, util
import os
import time
import chardet


class VocabLoader:
    @staticmethod
    def load_vocab(file_path, file_type):
        """æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼çš„è¯æ±‡åŠ è½½"""
        try:
            if file_type == "excel":
                # æ‰‹åŠ¨æŒ‡å®šå¼•æ“
                df = pd.read_excel(file_path, sheet_name=SHEET_NAME, engine='openpyxl')
                return df[COLUMN_NAME].tolist()
            elif file_type == "csv":
                with open(file_path, 'rb') as f:
                    rawdata = f.read()
                    # å¤šæ¬¡å°è¯•æ£€æµ‹ç¼–ç 
                    result = chardet.detect(rawdata)
                    encodings_to_try = [result['encoding'], 'gbk', 'utf-8', 'utf-16', 'cp936']
                    for encoding in encodings_to_try:
                        try:
                            df = pd.read_csv(file_path, encoding=encoding)
                            return df[COLUMN_NAME].tolist()
                        except UnicodeDecodeError:
                            continue
                    raise ValueError("æ— æ³•ç¡®å®šæ–‡ä»¶ç¼–ç ")
            elif file_type == "txt":
                with open(file_path, 'r', encoding='utf-8') as f:
                    return [line.strip() for line in f if line.strip()]
            else:
                raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè¯·é€‰æ‹© excel/csv/txt")
        except Exception as e:
            raise RuntimeError(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")


class SynonymDetector:
    def __init__(self, list_a, categories_b, sub_categories_b):
        self.model = SentenceTransformer('E:\\Sherry\\huggingface\\paraphrase-multilingual-MiniLM-L12-v2')
        self.embeddings_a = self.model.encode(list_a, convert_to_tensor=True)
        self.categories_b = categories_b
        self.sub_categories_b = sub_categories_b
        self.list_a = list_a

    def find_match(self, word_b, category, sub_category):
        emb_b = self.model.encode(word_b, convert_to_tensor=True)
        relevant_indices = [i for i, (cat, sub_cat) in enumerate(zip(self.categories_b, self.sub_categories_b))
                            if cat == category and sub_cat == sub_category]
        relevant_embeddings = self.embeddings_a[relevant_indices]
        relevant_words = [self.list_a[i] for i in relevant_indices]
        relevant_categories = [self.categories_b[i] for i in relevant_indices]
        relevant_sub_categories = [self.sub_categories_b[i] for i in relevant_indices]

        cos_scores = util.cos_sim(emb_b, relevant_embeddings)[0]
        max_score = np.max(cos_scores.numpy())
        max_index = np.argmax(cos_scores) if relevant_indices else -1

        if max_score > 0 and max_index >= 0:
            match_word = relevant_words[max_index]
            match_category = relevant_categories[max_index]
            match_sub_category = relevant_sub_categories[max_index]
            formatted_match = f"{match_category}+{match_sub_category}"
        else:
            formatted_match = "æ— ç›¸ä¼¼è¯"

        return formatted_match, round(float(max_score), 3)


def main():
    print("æ­£åœ¨åŠ è½½è¯æ±‡è¡¨...")

    # åŠ è½½è¯æ±‡è¡¨
    try:
        list_a = VocabLoader.load_vocab(A_FILE, FILE_TYPE)
        list_b = VocabLoader.load_vocab(B_FILE, FILE_TYPE)
    except Exception as e:
        print(f" é”™è¯¯ï¼š{str(e)}")
        return

    print(f"æˆåŠŸåŠ è½½ï¼š\nAè¡¨è¯æ±‡æ•°ï¼š{len(list_a)}\nBè¡¨è¯æ±‡æ•°ï¼š{len(list_b)}")

    # å‡è®¾è¡¨Bæœ‰ä¸€ä¸ªåä¸ºâ€œå¤§ç±»â€å’Œâ€œå°ç±»â€çš„åˆ—ï¼Œè¿™é‡Œå…ˆæ¨¡æ‹ŸåŠ è½½
    with open(B_FILE, 'rb') as f:
        rawdata = f.read()
        result = chardet.detect(rawdata)
        encodings_to_try = [result['encoding'], 'gbk', 'utf-8', 'utf-16', 'cp936']
        for encoding in encodings_to_try:
            try:
                df_b = pd.read_csv(B_FILE, encoding=encoding)
                categories_b = df_b["å¤§ç±»"].tolist()
                sub_categories_b = df_b["å°ç±»"].tolist()
                break
            except UnicodeDecodeError:
                continue
        else:
            print("æ— æ³•ç¡®å®šæ–‡ä»¶ç¼–ç ")
            return

    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = SynonymDetector(list_a, categories_b, sub_categories_b)
    results = []

    print("\nğŸ” å¼€å§‹åŒ¹é…åˆ†æ...")
    start = time.time()

    # æ‰¹é‡å¤„ç†
    for word, category, sub_category in zip(list_b, categories_b, sub_categories_b):
        match_word, score = detector.find_match(word, category, sub_category)
        results.append({
            "ç›®æ ‡è¯": word,
            "åŒ¹é…è¯": match_word,
            "ç›¸ä¼¼åº¦": score,
            "æ˜¯å¦åŒ¹é…": score > THRESHOLD
        })

    # ç”ŸæˆæŠ¥å‘Š
    df_result = pd.DataFrame(results)
    output_file = "å‰ç¥¥é‡ŒåŒ¹é…ç»“æœ_å°ç±».csv"
    df_result.to_csv(output_file, index=False, encoding='utf_8_sig')

    print(f"""
ğŸ‰ åˆ†æå®Œæˆï¼
â± è€—æ—¶ï¼š{time.time() - start:.1f}ç§’
ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{os.path.abspath(output_file)}
    """)


if __name__ == "__main__":
    main()
